{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "from rpy2.robjects.packages import importr\n",
    "import rpy2.robjects as ro\n",
    "from rpy2.robjects import pandas2ri\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import confusion_matrix, roc_curve, auc, plot_roc_curve\n",
    "from scipy.interpolate import interpn\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time, sys\n",
    "from IPython.display import clear_output\n",
    "\n",
    "def update_progress(progress):\n",
    "    bar_length = 20\n",
    "    if isinstance(progress, int):\n",
    "        progress = float(progress)\n",
    "    if not isinstance(progress, float):\n",
    "        progress = 0\n",
    "    if progress < 0:\n",
    "        progress = 0\n",
    "    if progress >= 1:\n",
    "        progress = 1\n",
    "\n",
    "    block = int(round(bar_length * progress))\n",
    "\n",
    "    clear_output(wait = True)\n",
    "    text = \"Progress: [{0}] {1:.1f}%\".format( \"#\" * block + \"-\" * (bar_length - block), progress * 100)\n",
    "    print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotConfusionMatrix(real,prev,dir,model):\n",
    "    \"\"\"Image for the confusion matrix\n",
    "    \n",
    "    Arguments:\n",
    "        real {list} -- Real labels\n",
    "        prev {list} -- Predicted labels\n",
    "        model {string} -- add the model name to the file name\n",
    "    \"\"\"\n",
    "    #print('>>> Creating figure...')\n",
    "\n",
    "    fig = plt.figure()\n",
    "    plt.plot([2,2,4]) #2,3,5\n",
    "    ax0 = plt.subplot(2, 2, 1) #2,3,1\n",
    "    ax0 = sns.heatmap(confusion_matrix(real[0],prev[0]), annot=True, cbar = False, cmap=sns.light_palette('#dea369'))\n",
    "    ax0.set(xlabel='Predicted', ylabel='Real')\n",
    "    ax0.title.set_text('Confusion Matrix Fold 1')\n",
    "    ax1 = plt.subplot(2, 2, 2) #2,3,2\n",
    "    ax1 = sns.heatmap(confusion_matrix(real[1],prev[1]), annot=True, cbar = False, cmap=sns.light_palette('#dea369'))\n",
    "    ax1.set(xlabel='Predicted', ylabel='Real')\n",
    "    ax1.title.set_text('Confusion Matrix Fold 2')\n",
    "    ax2 = plt.subplot(2, 2, 3) #2,3,3\n",
    "    ax2 = sns.heatmap(confusion_matrix(real[2],prev[2]), annot=True, cbar = False, cmap=sns.light_palette('#dea369'))\n",
    "    ax2.set(xlabel='Predicted', ylabel='Real')\n",
    "    ax2.title.set_text('Confusion Matrix Fold 3')\n",
    "    ax3 = plt.subplot(2, 2, 4) #2,3,4\n",
    "    ax3 = sns.heatmap(confusion_matrix(real[3],prev[3]), annot=True, cbar = False, cmap=sns.light_palette('#dea369'))\n",
    "    ax3.set(xlabel='Predicted', ylabel='Real')\n",
    "    ax3.title.set_text('Confusion Matrix Fold 4')\n",
    "    \n",
    "    fig.tight_layout()\n",
    "    #print('>>> Saving figure...')\n",
    "    plt.savefig('../../data/figures/{}/matrix_{}.png'.format(dir, model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotRocCurve(X,y, name, dir, names, best=False, plot=False):\n",
    "    \"\"\"Image for the ROC curve\n",
    "\n",
    "    Arguments:\n",
    "        X {numpy.ndarray} -- Dataset to train\n",
    "        y {numpy.ndarray} -- Labels for the dataset\n",
    "        model {string} -- add the model name to the file name\n",
    "    \"\"\"\n",
    "    #print(dir, name)\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle = True)\n",
    "    tprs, aucs, real, prev = [], [], [], []\n",
    "    mean_fpr = np.linspace(0, 1, 100)\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    for i, (train, test) in enumerate(skf.split(X, y)):\n",
    "        if name == 'svm': model, p = trainSvm(X[train], y[train], X[test], y[test], best)\n",
    "        elif name == 'tree': model, p = trainTree(X[train], y[train], X[test], y[test], best)\n",
    "        elif name == 'log': model, p = trainLog(X[train], y[train], X[test], y[test], best)\n",
    "        prev.append(p)\n",
    "        real.append(list(y[test]))\n",
    "        viz = plot_roc_curve(model, X[test], y[test],\n",
    "                         name='ROC fold {}'.format(i+1),\n",
    "                         alpha=0.3, lw=1, ax=ax)\n",
    "        interp_tpr = np.interp(mean_fpr, viz.fpr, viz.tpr)\n",
    "        interp_tpr[0] = 0.0\n",
    "        tprs.append(interp_tpr)\n",
    "        aucs.append(viz.roc_auc)\n",
    "        \n",
    "        f = open('auc.txt','a')\n",
    "        for a in aucs:\n",
    "            f.write(str(a)+'\\n')\n",
    "        f.close()\n",
    "\n",
    "    #print('>>> Creating figure...')\n",
    "    ax.plot([0, 1], [0, 1], linestyle='--', lw=2, color='r', label='Chance', alpha=.8)\n",
    "\n",
    "    mean_tpr = np.mean(tprs, axis=0)\n",
    "    mean_tpr[-1] = 1.0\n",
    "    mean_auc = auc(mean_fpr, mean_tpr)\n",
    "    std_auc = np.std(aucs)\n",
    "    ax.plot(mean_fpr, mean_tpr, color='b',\n",
    "            label=r'Mean ROC (AUC = %0.2f $\\pm$ %0.2f)' % (mean_auc, std_auc),\n",
    "            lw=2, alpha=.8)\n",
    "\n",
    "    std_tpr = np.std(tprs, axis=0)\n",
    "    tprs_upper = np.minimum(mean_tpr + std_tpr, 1)\n",
    "    tprs_lower = np.maximum(mean_tpr - std_tpr, 0)\n",
    "    ax.fill_between(mean_fpr, tprs_lower, tprs_upper, color='grey', alpha=.2,\n",
    "                    label=r'$\\pm$ 1 std. dev.')\n",
    "\n",
    "    ax.set(xlim=[-0.05, 1.05], ylim=[-0.05, 1.05], title=\"Receiver operating characteristic\")\n",
    "    ax.legend(loc=\"lower right\")\n",
    "    if plot==True:\n",
    "        plt.show()\n",
    "        print('>>> Saving figure...')\n",
    "        plt.savefig('../../data/figures/{}/roc_auc_{}.png'.format(dir, name))\n",
    "        print('Saved in ../../data/figures/{}/roc_auc_{}.png'.format(dir, name))\n",
    "        plotConfusionMatrix(real,prev, dir, name)\n",
    "    plt.close()\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainSvm(train_X,train_y,test_X,test_y,best_params):\n",
    "    \"\"\"Uses a SVM model to fit the data.\n",
    "\n",
    "    Arguments:\n",
    "        train_X {numpy.ndarray} -- 0.8% of the original dataset for training\n",
    "        train_y {numpy.ndarray} -- labels for the training data\n",
    "        test_X {numpy.ndarray} -- 0.2% of the original dataset for testing\n",
    "    \"\"\"\t\n",
    "    if not best_params:\n",
    "        #print('>>> Starting grid search...')\n",
    "        parameters = {'kernel':['linear','rbf','sigmoid'], \n",
    "                    'C':[0.25,0.4,0.5,0.55,0.75,1], \n",
    "                    'tol':[1e-3,1e-4,1e-5], \n",
    "                    'gamma':[25,50,75,100,150,'auto'], \n",
    "                    'degree':[1,2,3,5,10]}\n",
    "        svm_model = SVC()\n",
    "        grid = GridSearchCV(svm_model, parameters, cv = 5, scoring='f1')\n",
    "        grid_result = grid.fit(train_X, train_y)\n",
    "        best_params = grid_result.best_params_\n",
    "        print(best_params)\n",
    "    #print('>>> Creating model...')\n",
    "    svm_model = SVC(kernel=best_params[\"kernel\"],\n",
    "                    C=best_params[\"C\"], \n",
    "                    tol=best_params[\"tol\"], \n",
    "                    gamma=best_params[\"gamma\"], \n",
    "                    degree=best_params[\"degree\"])\n",
    "    svm_model = svm_model.fit(train_X, train_y)\n",
    "    prev = svm_model.predict(test_X)\n",
    "    \n",
    "    f = open('accuracy.txt','a')\n",
    "    f.write(str(accuracy_score(test_y,prev))+'\\n')\n",
    "    f.close()\n",
    " \n",
    "    \n",
    "    f = open('f1.txt','a')\n",
    "    f.write(str(f1_score(test_y,prev, average='macro'))+'\\n')\n",
    "    f.close()\n",
    "    \n",
    "    return svm_model, list(prev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainLog(train_X,train_y,test_X,test_y,best_params):\n",
    "    \"\"\"Uses a Logarithmic model to fit the data.\n",
    "\n",
    "    Arguments:\n",
    "        train_X {numpy.ndarray} -- 0.8% of the original dataset for training\n",
    "        train_y {numpy.ndarray} -- labels for the training data\n",
    "        test_X {numpy.ndarray} -- 0.2% of the original dataset for testing\n",
    "    \"\"\"\t\n",
    "    if not best_params:\n",
    "        #print('>>> Starting grid search...')\n",
    "        parameters = {'penalty': ['l1', 'l2'],\n",
    "                    'C':[0.25,0.4,0.5,0.55,0.75,1], \n",
    "                    'tol':[1e-3,1e-4,1e-5],\n",
    "                    'solver':['liblinear']}\n",
    "        log_model = LogisticRegression()\n",
    "        grid = GridSearchCV(log_model, parameters, cv = 5, scoring='f1')\n",
    "        grid_result = grid.fit(train_X, train_y)\n",
    "        best_params = grid_result.best_params_\n",
    "        print(best_params)\t\n",
    "    #print('>>> Creating model...')\n",
    "    log_model = LogisticRegression(penalty = best_params['penalty'],\n",
    "                                    C = best_params['C'], \n",
    "                                    tol = best_params['tol'],\n",
    "                                    solver = best_params['solver'])\n",
    "    log_model = log_model.fit(train_X, train_y)\n",
    "    prev = list(log_model.predict(test_X))\n",
    "    f = open('accuracy.txt','a')\n",
    "    f.write(str(accuracy_score(test_y,prev))+'\\n')\n",
    "    f.close()\n",
    " \n",
    "    \n",
    "    f = open('f1.txt','a')\n",
    "    f.write(str(f1_score(test_y,prev, average='macro'))+'\\n')\n",
    "    f.close()\n",
    "    return log_model, prev\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainTree(train_X,train_y,test_X,test_y,best_params):\n",
    "    \"\"\"Uses a tree model to fit the data.\n",
    "\n",
    "    Arguments:\n",
    "        train_X {numpy.ndarray} -- 0.8% of the original dataset for training\n",
    "        train_y {numpy.ndarray} -- labels for the training data\n",
    "        test_X {numpy.ndarray} -- 0.2% of the original dataset for testing\n",
    "    \"\"\"\t\n",
    "    if not best_params:\n",
    "        #print('>>> Starting grid search...')\n",
    "        parameters = {'n_estimators':[20,50,75,100], \n",
    "                    'criterion':['entropy','gini'], \n",
    "                    'min_samples_leaf':[1,2,3,5,10], \n",
    "                    'min_samples_split':[2,4,5,8,10], \n",
    "                    'max_leaf_nodes':[2,20,50,75,100]}\n",
    "        tree_model = ExtraTreesClassifier()\n",
    "        grid = GridSearchCV(tree_model, parameters, cv = 5, scoring='f1')\n",
    "        grid_result = grid.fit(train_X, train_y)\n",
    "        best_params = grid_result.best_params_\n",
    "        print(best_params)\t\n",
    "    #print('>>> Creating model...')\n",
    "    tree_model = ExtraTreesClassifier(criterion=best_params[\"criterion\"], \n",
    "                max_leaf_nodes=best_params[\"max_leaf_nodes\"], \n",
    "                min_samples_leaf=best_params[\"min_samples_leaf\"], \n",
    "                min_samples_split=best_params[\"min_samples_split\"], \n",
    "                n_estimators=best_params[\"n_estimators\"])\n",
    "    tree_model = tree_model.fit(train_X, train_y)\n",
    "    prev = tree_model.predict(test_X)\n",
    "    f = open('accuracy.txt','a')\n",
    "    f.write(str(accuracy_score(test_y,prev))+'\\n')\n",
    "    f.close()\n",
    " \n",
    "    \n",
    "    f = open('f1.txt','a')\n",
    "    f.write(str(f1_score(test_y,prev, average='macro'))+'\\n')\n",
    "    f.close()\n",
    "    return tree_model, list(prev)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifier(classi, dataset, name, grid=False, plot=False):    \n",
    "    X=dataset.iloc[:,:-1].values\n",
    "    y=pd.to_numeric(dataset.iloc[:,-1].values.ravel())\n",
    "    names=dataset.iloc[:,:-1].columns\n",
    "    for c in classi:\n",
    "        if c == 'svm': plotRocCurve(X, y, 'svm', name, names, grid, plot)\n",
    "        elif c == 'tree': plotRocCurve(X, y, 'tree', name, names, grid, plot)\n",
    "        elif c == 'log': plotRocCurve(X, y, 'log', name, names, grid, plot)\n",
    "        else: print('>>> The classifier chosen is not valid!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcMetrics(name, model):\n",
    "    metrics=['accuracy','f1','auc']\n",
    "    for m in metrics:\n",
    "        f = open('{}.txt'.format(m),'r')\n",
    "        met=[]\n",
    "        met = f.read().splitlines()\n",
    "        f.close()\n",
    "        met = [float(i) for i in met]\n",
    "        print('{} {} {}'.format(name, model, m), statistics.mean(met), ' +/- ', statistics.stdev(met))\n",
    "        os.remove('{}.txt'.format(m))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 0.75, 'degree': 1, 'gamma': 'auto', 'kernel': 'sigmoid', 'tol': 0.001}\n",
      "{'C': 1, 'degree': 1, 'gamma': 75, 'kernel': 'sigmoid', 'tol': 0.001}\n",
      "{'C': 0.25, 'degree': 1, 'gamma': 'auto', 'kernel': 'sigmoid', 'tol': 0.001}\n",
      "{'C': 0.25, 'degree': 1, 'gamma': 25, 'kernel': 'linear', 'tol': 0.001}\n",
      "{'C': 0.25, 'degree': 1, 'gamma': 25, 'kernel': 'linear', 'tol': 0.001}\n",
      "{'C': 0.25, 'degree': 1, 'gamma': 25, 'kernel': 'linear', 'tol': 0.001}\n",
      "{'C': 0.25, 'degree': 1, 'gamma': 25, 'kernel': 'linear', 'tol': 0.001}\n",
      "{'C': 0.25, 'degree': 1, 'gamma': 25, 'kernel': 'linear', 'tol': 0.001}\n",
      "{'C': 0.25, 'degree': 1, 'gamma': 25, 'kernel': 'linear', 'tol': 0.001}\n",
      "{'C': 0.25, 'degree': 1, 'gamma': 25, 'kernel': 'linear', 'tol': 0.001}\n",
      "{'C': 1, 'degree': 1, 'gamma': 'auto', 'kernel': 'rbf', 'tol': 0.001}\n",
      "{'C': 1, 'degree': 1, 'gamma': 'auto', 'kernel': 'rbf', 'tol': 0.001}\n",
      "{'C': 1, 'degree': 1, 'gamma': 'auto', 'kernel': 'rbf', 'tol': 0.001}\n",
      "{'C': 1, 'degree': 1, 'gamma': 'auto', 'kernel': 'rbf', 'tol': 0.001}\n",
      "{'C': 0.75, 'degree': 1, 'gamma': 'auto', 'kernel': 'rbf', 'tol': 0.001}\n",
      "{'C': 0.25, 'degree': 1, 'gamma': 25, 'kernel': 'linear', 'tol': 0.001}\n",
      "{'C': 0.25, 'degree': 1, 'gamma': 25, 'kernel': 'linear', 'tol': 0.001}\n",
      "{'C': 0.25, 'degree': 1, 'gamma': 25, 'kernel': 'linear', 'tol': 0.001}\n",
      "{'C': 0.25, 'degree': 1, 'gamma': 25, 'kernel': 'linear', 'tol': 0.001}\n",
      "{'C': 0.25, 'degree': 1, 'gamma': 25, 'kernel': 'linear', 'tol': 0.001}\n",
      "{'C': 0.5, 'degree': 1, 'gamma': 25, 'kernel': 'linear', 'tol': 0.001}\n",
      "{'C': 0.4, 'degree': 1, 'gamma': 25, 'kernel': 'linear', 'tol': 0.001}\n",
      "{'C': 0.25, 'degree': 1, 'gamma': 25, 'kernel': 'linear', 'tol': 0.001}\n",
      "{'C': 1, 'degree': 1, 'gamma': 25, 'kernel': 'linear', 'tol': 0.001}\n",
      "{'C': 0.25, 'degree': 1, 'gamma': 25, 'kernel': 'linear', 'tol': 0.001}\n"
     ]
    }
   ],
   "source": [
    "dataset = pd.read_csv('../../data/datasets/reduced_dataset_risk.csv.gz', compression = 'gzip')\n",
    "classifier(['svm'], dataset, 'risk')\n",
    "dataset = pd.read_csv('../../data/datasets/reduced_dataset_pval.csv.gz', compression = 'gzip')\n",
    "classifier(['svm'], dataset, 'all_pval')\n",
    "dataset = pd.read_csv('../../data/datasets/top_dataset_pval.csv.gz', compression = 'gzip')\n",
    "classifier(['svm'], dataset, 'pval')\n",
    "dataset = pd.read_csv('../../data/datasets/reduced_dataset_network.csv.gz', compression = 'gzip')\n",
    "classifier(['svm'], dataset, 'all_network')\n",
    "dataset = pd.read_csv('../../data/datasets/top_dataset_network.csv.gz', compression = 'gzip')\n",
    "classifier(['svm'], dataset, 'network')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'criterion': 'entropy', 'max_leaf_nodes': 100, 'min_samples_leaf': 10, 'min_samples_split': 4, 'n_estimators': 20}\n",
      "{'criterion': 'gini', 'max_leaf_nodes': 75, 'min_samples_leaf': 5, 'min_samples_split': 10, 'n_estimators': 20}\n",
      "{'criterion': 'gini', 'max_leaf_nodes': 100, 'min_samples_leaf': 3, 'min_samples_split': 5, 'n_estimators': 20}\n",
      "{'criterion': 'entropy', 'max_leaf_nodes': 100, 'min_samples_leaf': 3, 'min_samples_split': 4, 'n_estimators': 20}\n",
      "{'criterion': 'entropy', 'max_leaf_nodes': 50, 'min_samples_leaf': 10, 'min_samples_split': 2, 'n_estimators': 20}\n",
      "{'criterion': 'gini', 'max_leaf_nodes': 20, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 100}\n",
      "{'criterion': 'gini', 'max_leaf_nodes': 100, 'min_samples_leaf': 1, 'min_samples_split': 5, 'n_estimators': 50}\n",
      "{'criterion': 'entropy', 'max_leaf_nodes': 75, 'min_samples_leaf': 2, 'min_samples_split': 8, 'n_estimators': 20}\n",
      "{'criterion': 'entropy', 'max_leaf_nodes': 50, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 75}\n",
      "{'criterion': 'gini', 'max_leaf_nodes': 20, 'min_samples_leaf': 2, 'min_samples_split': 4, 'n_estimators': 75}\n",
      "{'criterion': 'entropy', 'max_leaf_nodes': 20, 'min_samples_leaf': 2, 'min_samples_split': 10, 'n_estimators': 75}\n",
      "{'criterion': 'entropy', 'max_leaf_nodes': 20, 'min_samples_leaf': 2, 'min_samples_split': 5, 'n_estimators': 50}\n",
      "{'criterion': 'entropy', 'max_leaf_nodes': 20, 'min_samples_leaf': 1, 'min_samples_split': 10, 'n_estimators': 75}\n",
      "{'criterion': 'gini', 'max_leaf_nodes': 50, 'min_samples_leaf': 1, 'min_samples_split': 5, 'n_estimators': 50}\n",
      "{'criterion': 'entropy', 'max_leaf_nodes': 50, 'min_samples_leaf': 1, 'min_samples_split': 10, 'n_estimators': 50}\n",
      "{'criterion': 'entropy', 'max_leaf_nodes': 20, 'min_samples_leaf': 10, 'min_samples_split': 5, 'n_estimators': 100}\n",
      "{'criterion': 'entropy', 'max_leaf_nodes': 20, 'min_samples_leaf': 10, 'min_samples_split': 5, 'n_estimators': 50}\n",
      "{'criterion': 'entropy', 'max_leaf_nodes': 20, 'min_samples_leaf': 3, 'min_samples_split': 10, 'n_estimators': 75}\n",
      "{'criterion': 'gini', 'max_leaf_nodes': 100, 'min_samples_leaf': 3, 'min_samples_split': 10, 'n_estimators': 20}\n",
      "{'criterion': 'gini', 'max_leaf_nodes': 20, 'min_samples_leaf': 5, 'min_samples_split': 2, 'n_estimators': 50}\n",
      "{'criterion': 'gini', 'max_leaf_nodes': 20, 'min_samples_leaf': 10, 'min_samples_split': 4, 'n_estimators': 20}\n",
      "{'criterion': 'entropy', 'max_leaf_nodes': 50, 'min_samples_leaf': 2, 'min_samples_split': 5, 'n_estimators': 20}\n",
      "{'criterion': 'gini', 'max_leaf_nodes': 100, 'min_samples_leaf': 10, 'min_samples_split': 5, 'n_estimators': 20}\n",
      "{'criterion': 'gini', 'max_leaf_nodes': 75, 'min_samples_leaf': 5, 'min_samples_split': 4, 'n_estimators': 20}\n",
      "{'criterion': 'entropy', 'max_leaf_nodes': 75, 'min_samples_leaf': 5, 'min_samples_split': 2, 'n_estimators': 20}\n"
     ]
    }
   ],
   "source": [
    "dataset = pd.read_csv('../../data/datasets/reduced_dataset_risk.csv.gz', compression = 'gzip')\n",
    "classifier(['tree'], dataset, 'risk')\n",
    "dataset = pd.read_csv('../../data/datasets/reduced_dataset_pval.csv.gz', compression = 'gzip')\n",
    "classifier(['tree'], dataset, 'all_pval')\n",
    "dataset = pd.read_csv('../../data/datasets/top_dataset_pval.csv.gz', compression = 'gzip')\n",
    "classifier(['tree'], dataset, 'pval')\n",
    "dataset = pd.read_csv('../../data/datasets/reduced_dataset_network.csv.gz', compression = 'gzip')\n",
    "classifier(['tree'], dataset, 'all_network')\n",
    "dataset = pd.read_csv('../../data/datasets/top_dataset_network.csv.gz', compression = 'gzip')\n",
    "classifier(['tree'], dataset, 'network')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 1, 'penalty': 'l1', 'solver': 'liblinear', 'tol': 0.001}\n",
      "{'C': 0.4, 'penalty': 'l1', 'solver': 'liblinear', 'tol': 0.001}\n",
      "{'C': 0.4, 'penalty': 'l1', 'solver': 'liblinear', 'tol': 0.001}\n",
      "{'C': 0.75, 'penalty': 'l1', 'solver': 'liblinear', 'tol': 0.001}\n",
      "{'C': 0.5, 'penalty': 'l1', 'solver': 'liblinear', 'tol': 0.001}\n",
      "{'C': 0.5, 'penalty': 'l2', 'solver': 'liblinear', 'tol': 0.001}\n",
      "{'C': 0.4, 'penalty': 'l2', 'solver': 'liblinear', 'tol': 0.001}\n",
      "{'C': 0.25, 'penalty': 'l2', 'solver': 'liblinear', 'tol': 0.001}\n",
      "{'C': 1, 'penalty': 'l2', 'solver': 'liblinear', 'tol': 0.0001}\n",
      "{'C': 1, 'penalty': 'l2', 'solver': 'liblinear', 'tol': 0.0001}\n",
      "{'C': 0.55, 'penalty': 'l1', 'solver': 'liblinear', 'tol': 0.001}\n",
      "{'C': 0.5, 'penalty': 'l1', 'solver': 'liblinear', 'tol': 0.001}\n",
      "{'C': 0.55, 'penalty': 'l1', 'solver': 'liblinear', 'tol': 0.001}\n",
      "{'C': 0.75, 'penalty': 'l1', 'solver': 'liblinear', 'tol': 0.001}\n",
      "{'C': 0.4, 'penalty': 'l1', 'solver': 'liblinear', 'tol': 0.001}\n",
      "{'C': 0.25, 'penalty': 'l1', 'solver': 'liblinear', 'tol': 0.0001}\n",
      "{'C': 0.4, 'penalty': 'l2', 'solver': 'liblinear', 'tol': 0.001}\n",
      "{'C': 0.75, 'penalty': 'l1', 'solver': 'liblinear', 'tol': 0.0001}\n",
      "{'C': 0.25, 'penalty': 'l2', 'solver': 'liblinear', 'tol': 0.001}\n",
      "{'C': 0.25, 'penalty': 'l1', 'solver': 'liblinear', 'tol': 0.001}\n",
      "{'C': 0.25, 'penalty': 'l1', 'solver': 'liblinear', 'tol': 0.001}\n",
      "{'C': 0.55, 'penalty': 'l1', 'solver': 'liblinear', 'tol': 0.001}\n",
      "{'C': 0.4, 'penalty': 'l1', 'solver': 'liblinear', 'tol': 0.001}\n",
      "{'C': 0.25, 'penalty': 'l1', 'solver': 'liblinear', 'tol': 0.001}\n",
      "{'C': 0.4, 'penalty': 'l1', 'solver': 'liblinear', 'tol': 0.001}\n"
     ]
    }
   ],
   "source": [
    "dataset = pd.read_csv('../../data/datasets/reduced_dataset_risk.csv.gz', compression = 'gzip')\n",
    "classifier(['log'], dataset, 'risk')\n",
    "dataset = pd.read_csv('../../data/datasets/reduced_dataset_pval.csv.gz', compression = 'gzip')\n",
    "classifier(['log'], dataset, 'all_pval')\n",
    "dataset = pd.read_csv('../../data/datasets/top_dataset_pval.csv.gz', compression = 'gzip')\n",
    "classifier(['log'], dataset, 'pval')\n",
    "dataset = pd.read_csv('../../data/datasets/reduced_dataset_network.csv.gz', compression = 'gzip')\n",
    "classifier(['log'], dataset, 'all_network')\n",
    "dataset = pd.read_csv('../../data/datasets/top_dataset_network.csv.gz', compression = 'gzip')\n",
    "classifier(['log'], dataset, 'network')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Risk Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: [####################] 100.0%\n",
      "risk svm accuracy 0.6428421299554304  +/-  0.08495134974086228\n",
      "risk svm f1 0.6259111215738928  +/-  0.08837585696857804\n",
      "risk svm auc 0.6731316177762342  +/-  0.10060498644413439\n"
     ]
    }
   ],
   "source": [
    "print('>>> Loading dataset...')\n",
    "dataset = pd.read_csv('../../data/datasets/reduced_dataset_risk.csv.gz', compression = 'gzip')\n",
    "print(dataset.shape)\n",
    "grid={'C': 0.25, 'degree': 1, 'gamma': 25, 'kernel': 'linear', 'tol': 0.001}\n",
    "for i in range(1000):\n",
    "    classifier(['svm'], dataset, 'risk', grid)\n",
    "    update_progress(i / 1000)\n",
    "\n",
    "update_progress(1)\n",
    "calcMetrics('risk','svm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TREE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: [####################] 100.0%\n",
      "risk tree accuracy 0.8817326984126984  +/-  0.05198986977458492\n",
      "risk tree f1 0.8732956323932237  +/-  0.05676332635558321\n",
      "risk tree auc 0.9655389146567718  +/-  0.02529392749822451\n"
     ]
    }
   ],
   "source": [
    "print('>>> Loading dataset...')\n",
    "dataset = pd.read_csv('../../data/datasets/reduced_dataset_risk.csv.gz', compression = 'gzip')\n",
    "print(dataset.shape)\n",
    "grid={'criterion': 'entropy', 'max_leaf_nodes': 50, 'min_samples_leaf': 3, 'min_samples_split': 5, 'n_estimators': 50}\n",
    "for i in range(1000):\n",
    "    classifier(['tree'], dataset, 'risk', grid)\n",
    "    update_progress(i / 1000)\n",
    "\n",
    "update_progress(1)\n",
    "calcMetrics('risk','tree')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LOG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: [####################] 100.0%\n",
      "risk log accuracy 0.7355712698412699  +/-  0.07010743274465464\n",
      "risk log f1 0.7213405574068545  +/-  0.07387970769501197\n",
      "risk log auc 0.7909953102453102  +/-  0.07500073484839828\n"
     ]
    }
   ],
   "source": [
    "print('>>> Loading dataset...')\n",
    "dataset = pd.read_csv('../../data/datasets/reduced_dataset_risk.csv.gz', compression = 'gzip')\n",
    "print(dataset.shape)\n",
    "grid={'C': 0.4, 'penalty': 'l1', 'solver': 'liblinear', 'tol': 0.001}\n",
    "for i in range(1000):\n",
    "    classifier(['log'], dataset, 'risk', grid)\n",
    "    update_progress(i / 1000)\n",
    "\n",
    "update_progress(1)\n",
    "calcMetrics('risk','log')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pval Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: [####################] 100.0%\n",
      "pval svm accuracy 0.8484365079365079  +/-  0.05785962758014378\n",
      "pval svm f1 0.8389795672247028  +/-  0.06191355172435989\n",
      "pval svm auc 0.9237215728715729  +/-  0.04360888116686274\n"
     ]
    }
   ],
   "source": [
    "print('>>> Loading dataset...')\n",
    "dataset = pd.read_csv('../../data/datasets/reduced_dataset_pval.csv.gz', compression = 'gzip')\n",
    "print(dataset.shape)\n",
    "grid={'C': 0.25, 'degree': 1, 'gamma': 25, 'kernel': 'linear', 'tol': 0.001}\n",
    "for i in range(1000):\n",
    "    classifier(['svm'], dataset, 'all_pval', grid)\n",
    "    update_progress(i / 1000)\n",
    "\n",
    "update_progress(1)\n",
    "calcMetrics('pval','svm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TREE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: [####################] 100.0%\n",
      "pval tree accuracy 0.9202925396825397  +/-  0.04444995324086781\n",
      "pval tree f1 0.9179562926419127  +/-  0.04515256689849303\n",
      "pval tree auc 0.9801480416408987  +/-  0.018361431439741054\n"
     ]
    }
   ],
   "source": [
    "print('>>> Loading dataset...')\n",
    "dataset = pd.read_csv('../../data/datasets/reduced_dataset_pval.csv.gz', compression = 'gzip')\n",
    "print(dataset.shape)\n",
    "grid={'criterion': 'entropy', 'max_leaf_nodes': 50, 'min_samples_leaf': 3, 'min_samples_split': 5, 'n_estimators': 50}\n",
    "for i in range(1000):\n",
    "    classifier(['tree'], dataset, 'all_pval', grid)\n",
    "    update_progress(i / 1000)\n",
    "\n",
    "update_progress(1)\n",
    "calcMetrics('pval','tree')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LOG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: [####################] 100.0%\n",
      "pval log accuracy 0.8467468253968254  +/-  0.061488865984817044\n",
      "pval log f1 0.837850921608183  +/-  0.06504052966754922\n",
      "pval log auc 0.9255630797773655  +/-  0.048364065946515364\n"
     ]
    }
   ],
   "source": [
    "print('>>> Loading dataset...')\n",
    "dataset = pd.read_csv('../../data/datasets/reduced_dataset_pval.csv.gz', compression = 'gzip')\n",
    "print(dataset.shape)\n",
    "grid={'C': 0.4, 'penalty': 'l1', 'solver': 'liblinear', 'tol': 0.001}\n",
    "for i in range(1000):\n",
    "    classifier(['log'], dataset, 'all_pval', grid)\n",
    "    update_progress(i / 1000)\n",
    "\n",
    "update_progress(1)\n",
    "calcMetrics('pval','log')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# top 25 Pval Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: [####################] 100.0%\n",
      "top_pval svm accuracy 0.7862357142857143  +/-  0.058723366712039655\n",
      "top_pval svm f1 0.7680483039580441  +/-  0.06640412382935759\n",
      "top_pval svm auc 0.8436645949288807  +/-  0.06764881912703213\n"
     ]
    }
   ],
   "source": [
    "print('>>> Loading dataset...')\n",
    "dataset = pd.read_csv('../../data/datasets/top_dataset_pval.csv.gz', compression = 'gzip')\n",
    "print(dataset.shape)\n",
    "grid={'C': 0.25, 'degree': 1, 'gamma': 25, 'kernel': 'linear', 'tol': 0.001}\n",
    "for i in range(1000):\n",
    "    classifier(['svm'], dataset, 'pval', grid)\n",
    "    update_progress(i / 1000)\n",
    "\n",
    "update_progress(1)\n",
    "calcMetrics('top_pval','svm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TREE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: [####################] 100.0%\n",
      "top_pval tree accuracy 0.8838312698412698  +/-  0.049810603540462316\n",
      "top_pval tree f1 0.8823329355823466  +/-  0.04967873685252635\n",
      "top_pval tree auc 0.9480752009894867  +/-  0.03438855335381645\n"
     ]
    }
   ],
   "source": [
    "print('>>> Loading dataset...')\n",
    "dataset = pd.read_csv('../../data/datasets/top_dataset_pval.csv.gz', compression = 'gzip')\n",
    "print(dataset.shape)\n",
    "grid={'criterion': 'entropy', 'max_leaf_nodes': 50, 'min_samples_leaf': 3, 'min_samples_split': 5, 'n_estimators': 50}\n",
    "for i in range(1000):\n",
    "    classifier(['tree'], dataset, 'pval', grid)\n",
    "    update_progress(i / 1000)\n",
    "\n",
    "update_progress(1)\n",
    "calcMetrics('top_pval','tree')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LOG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: [####################] 100.0%\n",
      "top_pval log accuracy 0.7812476190476191  +/-  0.05911068196468217\n",
      "top_pval log f1 0.7595383784474967  +/-  0.06848332853191773\n",
      "top_pval log auc 0.8536187487116058  +/-  0.06219496782102092\n"
     ]
    }
   ],
   "source": [
    "print('>>> Loading dataset...')\n",
    "dataset = pd.read_csv('../../data/datasets/top_dataset_pval.csv.gz', compression = 'gzip')\n",
    "print(dataset.shape)\n",
    "grid={'C': 0.4, 'penalty': 'l1', 'solver': 'liblinear', 'tol': 0.001}\n",
    "for i in range(1000):\n",
    "    classifier(['log'], dataset, 'pval', grid)\n",
    "    update_progress(i / 1000)\n",
    "\n",
    "update_progress(1)\n",
    "calcMetrics('top_pval','log')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Central Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: [####################] 100.0%\n",
      "central svm accuracy 0.7657236507936508  +/-  0.06624858862954339\n",
      "central svm f1 0.7529332547981656  +/-  0.07057365217012622\n",
      "central svm auc 0.8356863739435167  +/-  0.05601429441310791\n"
     ]
    }
   ],
   "source": [
    "print('>>> Loading dataset...')\n",
    "dataset = pd.read_csv('../../data/datasets/reduced_dataset_network.csv.gz', compression = 'gzip')\n",
    "print(dataset.shape)\n",
    "grid={'C': 0.25, 'degree': 1, 'gamma': 25, 'kernel': 'linear', 'tol': 0.001}\n",
    "for i in range(1000):\n",
    "    classifier(['svm'], dataset, 'all_network', grid)\n",
    "    update_progress(i / 1000)\n",
    "\n",
    "update_progress(1)\n",
    "calcMetrics('central','svm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TREE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: [####################] 100.0%\n",
      "central tree accuracy 0.9267109523809524  +/-  0.039235852060522936\n",
      "central tree f1 0.9198706630399279  +/-  0.044409578591601634\n",
      "central tree auc 0.9826765615337044  +/-  0.01773383594418541\n"
     ]
    }
   ],
   "source": [
    "print('>>> Loading dataset...')\n",
    "dataset = pd.read_csv('../../data/datasets/reduced_dataset_network.csv.gz', compression = 'gzip')\n",
    "print(dataset.shape)\n",
    "grid={'criterion': 'entropy', 'max_leaf_nodes': 50, 'min_samples_leaf': 3, 'min_samples_split': 5, 'n_estimators': 50}\n",
    "for i in range(1000):\n",
    "    classifier(['tree'], dataset, 'all_network', grid)\n",
    "    update_progress(i / 1000)\n",
    "\n",
    "update_progress(1)\n",
    "calcMetrics('central','tree')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LOG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: [####################] 100.0%\n",
      "central log accuracy 0.7925711111111111  +/-  0.06119135232879844\n",
      "central log f1 0.7818761225050833  +/-  0.06474569254771206\n",
      "central log auc 0.8703942589156874  +/-  0.0508216757168687\n"
     ]
    }
   ],
   "source": [
    "print('>>> Loading dataset...')\n",
    "dataset = pd.read_csv('../../data/datasets/reduced_dataset_network.csv.gz', compression = 'gzip')\n",
    "print(dataset.shape)\n",
    "grid={'C': 0.4, 'penalty': 'l1', 'solver': 'liblinear', 'tol': 0.001}\n",
    "for i in range(1000):\n",
    "    classifier(['log'], dataset, 'all_network', grid)\n",
    "    update_progress(i / 1000)\n",
    "\n",
    "update_progress(1)\n",
    "calcMetrics('central','log')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# top 25 Central Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: [####################] 100.0%\n",
      "top_central svm accuracy 0.9088252380952381  +/-  0.04266284713979564\n",
      "top_central svm f1 0.9026219805748964  +/-  0.046342393807136055\n",
      "top_central svm auc 0.9612027107812822  +/-  0.028323120925549186\n"
     ]
    }
   ],
   "source": [
    "print('>>> Loading dataset...')\n",
    "dataset = pd.read_csv('../../data/datasets/top_dataset_network.csv.gz', compression = 'gzip')\n",
    "print(dataset.shape)\n",
    "grid={'C': 0.25, 'degree': 1, 'gamma': 25, 'kernel': 'linear', 'tol': 0.001}\n",
    "for i in range(1000):\n",
    "    classifier(['svm'], dataset, 'network', grid)\n",
    "    update_progress(i / 1000)\n",
    "\n",
    "update_progress(1)\n",
    "calcMetrics('top_central','svm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TREE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: [####################] 100.0%\n",
      "top_central tree accuracy 0.9278220634920634  +/-  0.04279003020672129\n",
      "top_central tree f1 0.9214145252598337  +/-  0.04803132456256997\n",
      "top_central tree auc 0.9882001339929911  +/-  0.014431074443263026\n"
     ]
    }
   ],
   "source": [
    "print('>>> Loading dataset...')\n",
    "dataset = pd.read_csv('../../data/datasets/top_dataset_network.csv.gz', compression = 'gzip')\n",
    "print(dataset.shape)\n",
    "grid={'criterion': 'entropy', 'max_leaf_nodes': 50, 'min_samples_leaf': 3, 'min_samples_split': 5, 'n_estimators': 50}\n",
    "for i in range(1000):\n",
    "    classifier(['tree'], dataset, 'network', grid)\n",
    "    update_progress(i / 1000)\n",
    "\n",
    "update_progress(1)\n",
    "calcMetrics('top_central','tree')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LOG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: [####################] 100.0%\n",
      "top_central log accuracy 0.9140957142857142  +/-  0.044843361358720114\n",
      "top_central log f1 0.9094687418978469  +/-  0.04744716604757636\n",
      "top_central log auc 0.9664025767882911  +/-  0.02583824895028706\n"
     ]
    }
   ],
   "source": [
    "print('>>> Loading dataset...')\n",
    "dataset = pd.read_csv('../../data/datasets/top_dataset_network.csv.gz', compression = 'gzip')\n",
    "print(dataset.shape)\n",
    "grid={'C': 0.4, 'penalty': 'l1', 'solver': 'liblinear', 'tol': 0.001}\n",
    "for i in range(1000):\n",
    "    classifier(['log'], dataset, 'network', grid)\n",
    "    update_progress(i / 1000)\n",
    "\n",
    "update_progress(1)\n",
    "calcMetrics('top_central','log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
